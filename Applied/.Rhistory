summary(svmfit)
# Plot the SVM
x11()
plot(svmfit , dat, col =c('salmon', 'light blue'), pch=19, asp=1)
table(true=dat[train ,"y"], pred=predict (svmfit ,
newdata =dat[train ,]))
(3+4)/100
# Misclassification error on the test set
table(true=dat[-train ,"y"], pred=predict (svmfit ,
newdata =dat[-train ,]))
(3+10)/100
svmfit <- svm(y~., data=dat [train ,], kernel ='radial',gamma =1,cost=1e5)
x11()
plot(svmfit , dat, col =c('salmon', 'light blue'), pch=19, asp=1)
table(true=dat[train ,"y"], pred=predict (svmfit ,
newdata =dat[train ,]))
0
# Misclassification error on the test set
table(true=dat[-train ,"y"], pred=predict (svmfit ,
newdata =dat[-train ,]))
(5+20)/100
set.seed (1)
tune.out <- tune(svm , y~., data=dat[train ,], kernel ='radial',
ranges =list(cost=c(0.1 ,1 ,10 ,100 ,1000),
gamma=c(0.5,1,2,3,4) ))
summary(tune.out)
# Misclassification error with best model on the training set
table(true=dat[train ,"y"], pred=predict (tune.out$best.model ,
newdata =dat[train ,]))
(2+4)/100
# Misclassification error with best model on the test set
table(true=dat[-train ,"y"], pred=predict (tune.out$best.model ,
newdata =dat[-train ,]))
(2+10)/100
library (ISLR)
help(Khan)
install.packages("ISLR")
is(Khan)
library (ISLR)
help(Khan)
is(Khan)
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
length (Khan$ytrain)
length (Khan$ytest)
table(Khan$ytrain)
table(Khan$ytest)
dat <- data.frame(x=Khan$xtrain , y=as.factor(Khan$ytrain))
out <- svm(y~., data=dat , kernel ="linear",cost =10)
summary (out)
table(out$fitted , dat$y)
dat.te <- data.frame(x=Khan$xtest , y=as.factor (Khan$ytest ))
pred.te <- predict (out , newdata =dat.te)
table(pred.te , dat.te$y)
load("C:/Users/ffede/OneDrive/Desktop/AppliedStatTDE/Applied/mcshapiro.test.RData")
setwd("C:/Users/ffede/OneDrive/Desktop/AppliedStatTDE/Labs/Lab10-Clustering")
library(mvtnorm)
library(rgl)
library(car)
species.name <- iris[,5]
iris4        <- iris[,1:4]
x11()
pairs(iris4)
dev.off()
# compute the dissimilarity matrix of the data
# we choose the Euclidean metric (and then we look at other metrics)
help(dist)
iris.e <- dist(iris4, method='euclidean')
x11()
image(1:150,1:150,as.matrix(iris.e), main='metrics: Euclidean', asp=1, xlab='i', ylab='j')
# with other metrics:
iris.m <- dist(iris4, method='manhattan')
iris.c <- dist(iris4, method='canberra')
x11()
par(mfrow=c(1,3))
image(1:150,1:150,as.matrix(iris.e), main='metrics: Euclidean', asp=1, xlab='i', ylab='j' )
image(1:150,1:150,as.matrix(iris.c), main='metrics: Canberra', asp=1, xlab='i', ylab='j' )
image(1:150,1:150,as.matrix(iris.m), main='metrics: Manhattan', asp=1, xlab='i', ylab='j' )
# actually, the data are never ordered according to (unknown) labels
misc <- sample(150)
iris4 <- iris4[misc,]
iris.e <- dist(iris4, method='euclidean')
iris.m <- dist(iris4, method='manhattan')
iris.c <- dist(iris4, method='canberra')
x11()
par(mfrow=c(1,3))
image(1:150,1:150,as.matrix(iris.e), main='metrics: Euclidean', asp=1, xlab='i', ylab='j' )
image(1:150,1:150,as.matrix(iris.c), main='metrics: Canberra', asp=1, xlab='i', ylab='j' )
image(1:150,1:150,as.matrix(iris.m), main='metrics: Manhattan', asp=1, xlab='i', ylab='j' )
# Command hclust()
help(hclust)
iris.es <- hclust(iris.e, method='single')
iris.ea <- hclust(iris.e, method='average')
iris.ec <- hclust(iris.e, method='complete')
# if we want more detailed information on euclidean-complete
# clustering:
names(iris.ec)
iris.ec$merge  # order of aggregation of statistical units / clusters
iris.ec$height # distance at which we have aggregations
iris.ec$order  # ordering that allows to avoid intersections in the dendrogram
# plot of the dendrograms
x11()
par(mfrow=c(1,3))
plot(iris.es, main='euclidean-single', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(iris.ec, main='euclidean-complete', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(iris.ea, main='euclidean-average', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
x11()
par(mfrow=c(1,3))
plot(iris.es, main='euclidean-single', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(iris.es, k=2)
plot(iris.ec, main='euclidean-complete', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(iris.ec, k=2)
plot(iris.ea, main='euclidean-average', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(iris.ea, k=2)
x11()
par(mfrow=c(1,3))
plot(iris.es, main='euclidean-single', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(iris.es, k=2)
rect.hclust(iris.es, k=3)
plot(iris.ec, main='euclidean-complete', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(iris.ec, k=2)
rect.hclust(iris.ec, k=3)
plot(iris.ea, main='euclidean-average', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(iris.ea, k=2)
rect.hclust(iris.ea, k=3)
help(cutree)
cluster.ec <- cutree(iris.ec, k=2) # euclidean-complete:
cluster.ec
cluster.es <- cutree(iris.es, k=2) # euclidean-single
cluster.ea <- cutree(iris.ea, k=2) # euclidean-average
# Let's give a mark to the algorithms: did they aggregate coherently with
# the dissimilarity matrix or not?
# compute the cophenetic matrices
coph.es <- cophenetic(iris.es)
coph.ec <- cophenetic(iris.ec)
coph.ea <- cophenetic(iris.ea)
x11()
layout(rbind(c(0,1,0),c(2,3,4)))
image(as.matrix(iris.e), main='Euclidean', asp=1 )
image(as.matrix(coph.es), main='Single', asp=1 )
image(as.matrix(coph.ec), main='Complete', asp=1 )
image(as.matrix(coph.ea), main='Average', asp=1 )
es <- cor(iris.e, coph.es)
ec <- cor(iris.e, coph.ec)
ea <- cor(iris.e, coph.ea)
c("Eucl-Single"=es,"Eucl-Compl."=ec,"Eucl-Ave."=ea)
# interpret the clusters
table(label.true = species.name[misc], label.cluster = cluster.es)
table(label.true = species.name[misc], label.cluster = cluster.ec)
table(label.true = species.name[misc], label.cluster = cluster.ea)
x11()
plot(iris4, col=ifelse(cluster.es==1,'red','blue'), pch=19)
x11()
plot(iris4, col=ifelse(cluster.ec==1,'red','blue'), pch=19)
x11()
plot(iris4, col=ifelse(cluster.ea==1,'red','blue'), pch=19)
set.seed(123)
# x : vector of NON clustered data
x <- 0:124/124 + rnorm(125, sd=0.01)
# y : vector of clustered data (5 clusters)
y <- c(rnorm(25, mean=0, sd=0.01), rnorm(25, mean=0.25, sd=0.01),
rnorm(25, mean=0.5, sd=0.01), rnorm(25, mean=0.75, sd=0.01),
rnorm(25, mean=1, sd=0.01))
x <- sample(x)
y <- sample(y)
x11()
par(mfrow=c(1,2))
plot(rep(0,125),x, main='data: no clust',xlab='')
plot(rep(0,125),y, main='data: clust',xlab='')
dx <- dist(x)
dy <- dist(y)
hcx<- hclust(dx, method='single')
hcy<- hclust(dy, method='single')
x11()
par(mfrow=c(1,2))
plot(hcx, labels=F, cex=0.5, hang=-0.1, xlab='', sub='x')
plot(hcy, labels=F, cex=0.5, hang=-0.1, xlab='', sub='y')
x11()
par(mfrow=c(2,2))
image(as.matrix(dx), asp=1, main='not ordered x' )
image(as.matrix(dy), asp=1, main='not ordered y' )
image(as.matrix(dx)[hcx$order,hcx$order], asp=1, main='reordered x' )
image(as.matrix(dy)[hcy$order,hcy$order], asp=1, main='reordered y' )
p <- 2
n <- 100
mu1 <- c(0,1)
mu2 <- c(5,1.2)
sig <- diag(rep(1,p))
set.seed(1)
X1 <- rmvnorm(n, mu1, sig)
X2 <- rmvnorm(n, mu2, sig)
X <- rbind(X1, X2)
# If we knew the labels:
x11()
plot(X, xlab='Var 1', ylab='Var 2', col=rep(c('red','blue'),each=100), asp=1)
plot(X, xlab='Var 1', ylab='Var 2', asp=1)
x.d <- dist(X, method = 'euclidean')
x.es <- hclust(x.d, method='single')
x.ea <- hclust(x.d, method='average')
x.ec <- hclust(x.d, method='complete')
x11()
par(mfrow=c(1,3))
plot(x.es, main = 'Single linkage'  , hang=-0.1, xlab='', labels=F, sub='')
plot(x.ea, main = 'Average linkage' , hang=-0.1, xlab='', labels=F, sub='')
plot(x.ec, main = 'Complete linkage', hang=-0.1, xlab='', labels=F, sub='')
cluster.es <- cutree(x.es, k = 2)
cluster.ea <- cutree(x.ea, k = 2)
cluster.ec <- cutree(x.ec, k = 2)
x11()
par(mfrow=c(1,3))
plot(X, xlab='Var 1', ylab='Var 2', main = 'Single linkage', col=ifelse(cluster.es==1,'red','blue'), pch=16, asp=1)
plot(X, xlab='Var 1', ylab='Var 2', main = 'Average linkage', col=ifelse(cluster.ea==1,'red','blue'), pch=16, asp=1)
plot(X, xlab='Var 1', ylab='Var 2', main = 'Complete linkage', col=ifelse(cluster.ec==1,'red','blue'), pch=16, asp=1)
p <- 2
n <- 100
mu1 <- c(0,1)
mu2 <- c(6.5,1)
e1 <- c(1,1)
e2 <- c(-1,1)
sig <- 5*cbind(e1)%*%rbind(e1)+.1*cbind(e2)%*%rbind(e2)
set.seed(2)
X1 <- rmvnorm(n, mu1, sig)
X2 <- rmvnorm(n, mu2, sig)
X <- rbind(X1, X2)
# If we knew the labels:
x11()
plot(X, xlab='Var 1', ylab='Var 2', asp=1, col=rep(c('red','blue'),each=100))
plot(X, xlab='Var 1', ylab='Var 2', asp=1)
x.d <- dist(X, method = 'euclidean')
x.es <- hclust(x.d, method='single')
x.ea <- hclust(x.d, method='average')
x.ec <- hclust(x.d, method='complete')
x11()
par(mfrow=c(1,3))
plot(x.es, main = 'Single linkage', ylab='Euclidean distance', hang=-0.1, xlab='', labels=F, sub='')
plot(x.ea, main = 'Average linkage', hang=-0.1, xlab='', labels=F, sub='')
plot(x.ec, main = 'Complete linkage', hang=-0.1, xlab='', labels=F, sub='')
# let's cut the tree to get 2 clusters
x11()
par(mfrow=c(1,3))
plot(x.es, main = 'Single linkage', ylab='Euclidean distance', hang=-0.1, xlab='', labels=F, sub='')
rect.hclust(x.es, k=2)
plot(x.ea, main = 'Average linkage', hang=-0.1, xlab='', labels=F, sub='')
rect.hclust(x.ea, k=2)
plot(x.ec, main = 'Complete linkage', hang=-0.1, xlab='', labels=F, sub='')
rect.hclust(x.ec, k=2)
cluster.es <- cutree(x.es, k = 2)
cluster.ea <- cutree(x.ea, k = 2)
cluster.ec <- cutree(x.ec, k = 2)
x11()
par(mfrow=c(1,3))
plot(X, xlab='Var 1', ylab='Var 2', main = 'Euclidean, Single linkage', col=cluster.es+1, pch=16, asp=1)
plot(X, xlab='Var 1', ylab='Var 2', main = 'Euclidean, Average linkage', col=cluster.ea+1, pch=16, asp=1)
plot(X, xlab='Var 1', ylab='Var 2', main = 'Euclidean, Complete linkage', col=cluster.ec+1, pch=16, asp=1)
help(quakes)
head(quakes)
dim(quakes)
Q <- cbind(quakes[,1:2], depth = -quakes[,3]/100)
head(Q)
plot3d(Q, size=3, col='orange', aspect = F)
# dissimilarity matrix (Euclidean metric)
d <- dist(Q)
x11()
image(as.matrix(d))
x11()
par(mfrow=c(2,2))
clusts <- hclust(d, method='single')
plot(clusts, hang=-0.1, labels=FALSE, main='single', xlab='', sub='')
# rect.hclust(clusts, k=2)
# rect.hclust(clusts, k=3)
clusta <- hclust(d, method='average')
plot(clusta, hang=-0.1, labels=FALSE, main='average', xlab='', sub='')
# rect.hclust(clusta, k=2)
# rect.hclust(clusta, k=3)
clustc <- hclust(d, method='complete')
plot(clustc, hang=-0.1, labels=FALSE, main='complete', xlab='', sub='')
# rect.hclust(clustc, k=2)
# rect.hclust(clustc, k=3)
clustw <- hclust(d, method='ward.D2')
plot(clustw, hang=-0.1, labels=FALSE, main='ward', xlab='', sub='')
# rect.hclust(clustw, k=2)
# rect.hclust(clustw, k=3)
open3d()
# single linkage
clusters <- cutree(clusts, 2)
plot3d(Q, size=3, col=clusters+1, aspect = F)
clusters <- cutree(clusts, 3)
plot3d(Q, size=3, col=clusters+1, aspect = F)
# average linkage
clustera <- cutree(clusta, 2)
plot3d(Q, size=3, col=clustera+1, aspect = F)
clustera <- cutree(clusta, 3)
plot3d(Q, size=3, col=clustera+1, aspect = F)
# complete linkage
clusterc <- cutree(clustc, 2)
plot3d(Q, size=3, col=clusterc+1, aspect = F)
clusterc <- cutree(clustc, 3)
plot3d(Q, size=3, col=clusterc+1, aspect = F)
# ward linkage
clusterw <- cutree(clustw, 2)
plot3d(Q, size=3, col=clusterw+1, aspect = F)
clusterw <- cutree(clustw, 3)
plot3d(Q, size=3, col=clusterw+1, aspect = F)
clusters <- cutree(clusts, 2)
plot3d(Q, size=3, col=clusters+1, aspect = F)
clusters <- cutree(clusts, 3)
plot3d(Q, size=3, col=clusters+1, aspect = F)
clustera <- cutree(clusta, 2)
plot3d(Q, size=3, col=clustera+1, aspect = F)
clustera <- cutree(clusta, 3)
plot3d(Q, size=3, col=clustera+1, aspect = F)
clusterc <- cutree(clustc, 2)
plot3d(Q, size=3, col=clusterc+1, aspect = F)
clusterc <- cutree(clustc, 3)
clusterc <- cutree(clustc, 3)
plot3d(Q, size=3, col=clusterc+1, aspect = F)
clusterw <- cutree(clustw, 2)
plot3d(Q, size=3, col=clusterw+1, aspect = F)
clusterw <- cutree(clustw, 3)
plot3d(Q, size=3, col=clusterw+1, aspect = F)
n <- 100
set.seed(1)
x <- matrix(rnorm(n*2), ncol=2)
x[1:(n/2),1] <- x[1:(n/2),1]+2
x[1:(n/2),2] <- x[1:(n/2),2]-2
x11()
plot(x,pch=20,cex=2,xlab='x1',ylab='x2')
k <- 2
cluster <- sample(1:2, n, replace=TRUE)
iter.max <- 3
colplot <- c('royalblue','red')
colpoints <- c('blue4','red4')
x11()
par(mfrow = c(iter.max,3))
for(i in 1:iter.max)
{
C <- NULL
for(l in 1:k)
C <- rbind(C, colMeans(x[cluster == l,]))
plot(x, col = colplot[cluster],pch=19)
line <- readline()
points(C, col = colpoints, pch = 4, cex = 2, lwd = 2)
line <- readline()
plot(x, col = 'grey',pch=19)
points(C, col = colpoints, pch = 4, cex = 2, lwd = 2)
line <- readline()
QC <- rbind(C, x)
Dist <- as.matrix(dist(QC, method = 'euclidean'))[(k+1):(k+n),1:k]
for(j in 1:n)
cluster[j] <- which.min(Dist[j,])
plot(x, col = colplot[cluster],pch=19)
points(C, col = colpoints, pch = 4, cex = 2, lwd = 2)
line <- readline()
}
x11()
par(mfrow = c(iter.max,3))
for(i in 1:iter.max)
{
C <- NULL
for(l in 1:k)
C <- rbind(C, colMeans(x[cluster == l,]))
plot(x, col = colplot[cluster],pch=19)
line <- readline()
points(C, col = colpoints, pch = 4, cex = 2, lwd = 2)
line <- readline()
plot(x, col = 'grey',pch=19)
points(C, col = colpoints, pch = 4, cex = 2, lwd = 2)
line <- readline()
QC <- rbind(C, x)
Dist <- as.matrix(dist(QC, method = 'euclidean'))[(k+1):(k+n),1:k]
for(j in 1:n)
cluster[j] <- which.min(Dist[j,])
plot(x, col = colplot[cluster],pch=19)
points(C, col = colpoints, pch = 4, cex = 2, lwd = 2)
line <- readline()
}
result.k$size         # dimention of the clusters
x11()
plot(Q, col = result.k$cluster+1)
# back to the earthquakes dataset
### in automatic, command kmeans()
help(kmeans)
result.k <- kmeans(Q, centers=2) # Centers: fixed number of clusters
names(result.k)
result.k$cluster      # labels of clusters
result.k$centers      # centers of the clusters
result.k$totss        # tot. sum of squares
result.k$withinss     # sum of squares within clusters
result.k$tot.withinss # sum(sum of squares nei cluster)
result.k$betweenss    # sum of squares between clusters
result.k$size         # dimention of the clusters
x11()
plot(Q, col = result.k$cluster+1)
open3d()
plot3d(Q, size=3, col=result.k$cluster+1, aspect = F)
points3d(result.k$centers,size=10)
# method 1)
b <- NULL
w <- NULL
for(k in 1:10){
result.k <- kmeans(Q, k)
w <- c(w, sum(result.k$wit))
b <- c(b, result.k$bet)
}
x11()
matplot(1:10, w/(w+b), pch='', xlab='clusters', ylab='within/tot', main='Choice of k', ylim=c(0,1))
lines(1:10, w/(w+b), type='b', lwd=2)
result.k <- kmeans(Q, 3)
x11()
plot(Q, col = result.k$cluster+1)
open3d()
plot3d(Q, size=3, col=result.k$cluster+1, aspect = F)
points3d(result.k$centers, size = 10)
vowels <- read.table('veritatis.txt', header=T)
head(vowels)
dim(vowels)
### question a)
x11()
plot(vowels)
HC <- hclust(dist(vowels, method='manhattan'), method = 'average')
x11()
plot(HC, hang=-0.1, sub='', labels=F, xlab='')
rect.hclust(HC, k=2)
pag <- cutree(HC, k=2)
table(pag)
which(pag==2)
x11()
plot(vowels , col=pag+1, asp=1, pch=16, lwd=2)
### question b)
p  <- 5
n1 <- table(pag)[1]
n2 <- table(pag)[2]
# Verify gaussianity
load("D:/RTDA/Didattica/Applied Statistics MATE 19-20/Lab 5 - 16042020/mcshapiro.test.RData")
mcshapiro.test(vowels[pag=='1',])
mcshapiro.test(vowels[pag=='2',])
# Test for independent Gaussian populations
t1.mean <- sapply(vowels[pag=='1',],mean)
t2.mean <- sapply(vowels[pag=='2',],mean)
t1.cov  <-  cov(vowels[pag=='1',])
t2.cov  <-  cov(vowels[pag=='2',])
Sp      <- ((n1-1)*t1.cov + (n2-1)*t2.cov)/(n1+n2-2)
# Test: H0: mu.1-mu.2==0 vs H1: mu.1-mu.2!=0
delta.0 <- c(0,0,0,0,0)
Spinv   <- solve(Sp)
T2 <- n1*n2/(n1+n2) * (t1.mean-t2.mean-delta.0) %*% Spinv %*% (t1.mean-t2.mean-delta.0)
P <- 1 - pf(T2/(p*(n1+n2-2)/(n1+n2-1-p)), p, n1+n2-1-p)
P
### question c)
alpha <- 0.1
IC <- cbind(t2.mean-t1.mean - sqrt(diag(Sp)*(1/n1+1/n2)) * qt(1 - alpha/(p*2), n1+n2-2),
t2.mean-t1.mean,
t2.mean-t1.mean + sqrt(diag(Sp)*(1/n1+1/n2)) * qt(1 - alpha/(p*2), n1+n2-2))
IC
satellite <- read.table('satellite.txt', header=T)
head(satellite)
x11()
plot(satellite, asp = 1, pch=16)
D.s <- dist(satellite)
HCa <- hclust(D.s, method = 'average')
x11()
plot(HCa, hang=-0.1, sub='', xlab='', labels=F)
rect.hclust(HCa, k=2)
sata <- cutree(HCa, k=2)
x11()
plot(satellite , col=sata+1, asp=1, pch=16, main='Average'
)
table(sata)
coph.a <- cophenetic(HCa)
coph.sat <- cor(D.s, coph.a)
coph.sat
p  <- 2
n1 <- table(sata)[1]
n2 <- table(sata)[2]
# Assumptions:
# - Normality
# - Independent populations
# - Homogeneity of covariance structures
# Verificy Gaussian assumption
mcshapiro.test(satellite[sata=='1',])$pvalue
mcshapiro.test(satellite[sata=='2',])$pvalue
t1.mean <- sapply(satellite[sata=='1',],mean)
t2.mean <- sapply(satellite[sata=='2',],mean)
t1.cov  <-  cov(satellite[sata=='1',])
t2.cov  <-  cov(satellite[sata=='2',])
# Homogeneity of covariances
t1.cov
t2.cov
Sp      <- ((n1-1)*t1.cov + (n2-1)*t2.cov)/(n1+n2-2)
# Elliptic confidence region at 99%
alpha <- 0.01
cfr.fisher <- (p*(n1+n2-2)/(n1+n2-1-p))*qf(1-alpha, p, n1+n2-1-p)
# Characterize the ellipse:
# Directions of the axes
eigen(Sp)$vector
# Radius
r <- sqrt(cfr.fisher)
# Length of the semi-axes
r*sqrt(eigen(Sp)$values*(1/n1+1/n2))
x11(width=14, height = 7)
par(mfrow=c(1,2))
plot(satellite , col=sata+1, asp=1, pch=16, main='Original data and groups')
plot(satellite, xlim=c(-23,-15), ylim=c(-25,-17), pch='', asp=1,
main='Elliptic region for the mean diff. (red - green)')
# confidence region and sample mean in blue
ellipse(center=t1.mean-t2.mean, shape=Sp*(1/n1+1/n2), radius=sqrt(cfr.fisher),
lwd=2, col='blue')
graphics.off()
??roc
?density
?plot
